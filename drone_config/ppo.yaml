behaviors:
  My Behavior:
    trainer_type: ppo

    hyperparameters:
      # Hyperparameters common to PPO and SAC
      batch_size: 1024 # 연속적인 행동 공간 (Continuous Action Space) 환경을 사용하는 경우 이 값은 크게 설
      buffer_size: 10240 # batch_size의 배수로 설정되어야 합니다. 일반적으로 큰 buffer_size는 더 안정적인 학습을 가능하게 합니다.
      learning_rate: 3.0e-4 # 학습이 불안정하고 에이전트가 얻는 보상이 증가하지 않는 경우 일반적으로 학습률을 감소시킵니다.

      # PPO-specific hyperparameters
      beta: 5.0e-3 # 엔트로피 (텐서보드를 통해 측정 가능)는 보상이 증가함에 따라 서서히 크기를 감소시켜야합니다. 만약 엔트로피가 너무 빠르게 떨어지면 beta를 증가시켜야합니다. 만약 엔트로피가 너무 느리게 떨어지면 beta를 감소시켜야 합니다.
      epsilon: 0.2
      lambd: 0.95 #  Low values correspond to relying more on the current value estimate (which can be high bias), and high values correspond to relying more on the actual rewards received in the environment (which can be high variance).
      num_epoch: 3 # batch_size가 클수록 이 값도 커져야합니다. 이 값을 줄이면 더 안정적인 업데이트가 보장되지만 학습 속도가 느려집니다.

    # Configuration of the neural network (common to PPO/SAC)
    network_settings:
      vis_encode_type: simple # nature_cnn, resnet으로 바꾸는 것 고려
      normalize: true # 복잡한 문제여서 true로 바꾸는 것 고려
      hidden_units: 256 # 최적의 행동이 관측 입력의 복잡한 관계에 의해 결정되는 어려운 문제에 대해서는 이 값을 크게 설정
      num_layers: 3 # 복잡한 제어 문제에서는 많은 층을 사용할 필요

    # Trainer configurations common to all trainers
    max_steps: 1.0e7 # 복잡한 문제일수록 크게. (5e5 - 1e7)
    time_horizon: 512 # 한 에피소드 동안 보상이 빈번하게 발생하는 경우나 에피소드가 엄청나게 긴 경우에는 time horizon 값은 작게 설정하는 것이 이상적입니다. 이 값은 에이전트가 취하는 일련의 행동 내에서 중요한 행동을 모두 포착할 수 있을 만큼 큰 값(32 - 2048)
    summary_freq: 5000
    keep_checkpoints: 5
    checkpoint_interval: 50000
    init_path: null

    reward_signals:
      # environment reward (default)
      extrinsic:
        strength: 1.0
        gamma: 0.99
        network_settings:
          normalize: True

      # curiosity module
      rnd:
        strength: 1
        gamma: 0.99
        learning_rate: 3.0e-4
        network_settings:
          normalize: True
          hidden_units: 256
          num_layers: 3

